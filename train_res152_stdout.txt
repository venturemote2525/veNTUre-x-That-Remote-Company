Configuration: swiss_7class_resnet152
  Classes: 5 (['carb', 'meat', 'vegetable', 'others'])
  Data root: D:\githib repo clones\food-portion-size-classifier\src\models\pytorch_mask_rcnn\data\rte8_coco_quick
  Epochs: 2 (heads: 1)
  Batch size: 3
  Learning rate: 0.001
  Backbone: resnet152

=== RTX 3060 2-Stage Mask R-CNN Training ===
Backbone: resnet152 (ResNet-50 for better accuracy)
Batch Size: 3 (optimized for 12GB VRAM)
Workers: 4 (parallel data loading)
Total Epochs: 2
Training Type: 2-Stage (Heads-only + Full fine-tuning)
Time Limit: 2 hours
RTX 3060 12GB VRAM: Maximum performance settings

7 Food Classes: ['carb', 'meat', 'vegetable', 'others']
Total classes (including background): 5
Training logs will be saved to: logs\swiss_7class_resnet152\training_20250831_203131.log
Initializing trainer on cuda
Config: swiss_7class_resnet152
Building Mask R-CNN with RESNET152 backbone...
Total parameters: 78,476,013
Trainable parameters: 78,253,613
Setting up data loaders...
Using train split (mapped to train): D:\githib repo clones\food-portion-size-classifier\src\models\pytorch_mask_rcnn\data\rte8_coco_quick\train\annotations\instances.json
Loading train annotations from D:\githib repo clones\food-portion-size-classifier\src\models\pytorch_mask_rcnn\data\rte8_coco_quick\train\annotations\instances.json
Loaded 100 images
Classes: ['background', 'carb', 'meat', 'vegetable', 'others']
Using val split (mapped to test): D:\githib repo clones\food-portion-size-classifier\src\models\pytorch_mask_rcnn\data\rte8_coco_quick\test\annotations\instances.json
Loading val annotations from D:\githib repo clones\food-portion-size-classifier\src\models\pytorch_mask_rcnn\data\rte8_coco_quick\test\annotations\instances.json
Loaded 100 images
Classes: ['background', 'carb', 'meat', 'vegetable', 'others']
Training samples: 100
Validation samples: 100
Optimizer: SGD
Learning rate: 0.001
Scheduler: step

==================================================
Starting Food Mask R-CNN Two-Stage Training
==================================================

==================== STAGE 1: HEADS ONLY ====================
Freezing backbone layers for heads-only training...
Trainable parameters (heads only): 17,139,245
Training heads for 1 epochs with LR: 0.001

Stage 1 - Epoch 1/1
------------------------------
Epoch [1/2], Batch [5/34] (14.7%), Loss: 3.7461, Avg Loss: 3.8506
Epoch [1/2], Batch [10/34] (29.4%), Loss: 3.4002, Avg Loss: 3.6297
Epoch [1/2], Batch [15/34] (44.1%), Loss: 3.2878, Avg Loss: 3.5915
Epoch [1/2], Batch [20/34] (58.8%), Loss: 3.5320, Avg Loss: 3.5638
Epoch [1/2], Batch [25/34] (73.5%), Loss: 3.5334, Avg Loss: 3.5839
Epoch [1/2], Batch [30/34] (88.2%), Loss: 3.3930, Avg Loss: 3.5846

==================================================
EPOCH 1/2 COMPLETED
Average Training Loss: 3.5501
Processed Batches: 34/34
Success Rate: 100.0%
==================================================
Debug: Max score: 0.2700, Valid predictions: 100
Validation - Loss: 3.3185, mAP: 0.0000
Stage 1 Summary:
  Train Loss: 3.5501
  Val Loss: 3.3185
  Val mAP: 0.0000
  Best mAP: 0.0000
  Learning Rate: 0.001000
  Elapsed Time: 0.01h

==================== STAGE 2: ALL LAYERS ====================
Unfreezing all layers for full fine-tuning...
Trainable parameters (all layers): 78,476,013
Reducing batch size from 3 to 1 for Stage 2
Using train split (mapped to train): D:\githib repo clones\food-portion-size-classifier\src\models\pytorch_mask_rcnn\data\rte8_coco_quick\train\annotations\instances.json
Loading train annotations from D:\githib repo clones\food-portion-size-classifier\src\models\pytorch_mask_rcnn\data\rte8_coco_quick\train\annotations\instances.json
Loaded 100 images
Classes: ['background', 'carb', 'meat', 'vegetable', 'others']
Fine-tuning all layers for 1 epochs with LR: 0.0001

Stage 2 - Epoch 2/2
------------------------------
Epoch [2/2], Batch [5/100] (5.0%), Loss: 3.6182, Avg Loss: 3.3547
Epoch [2/2], Batch [10/100] (10.0%), Loss: 3.5640, Avg Loss: 3.3381
Epoch [2/2], Batch [15/100] (15.0%), Loss: 3.1417, Avg Loss: 3.4215
Epoch [2/2], Batch [20/100] (20.0%), Loss: 2.7954, Avg Loss: 3.3955
Epoch [2/2], Batch [25/100] (25.0%), Loss: 3.4462, Avg Loss: 3.3631
Epoch [2/2], Batch [30/100] (30.0%), Loss: 2.6054, Avg Loss: 3.3158
Epoch [2/2], Batch [35/100] (35.0%), Loss: 3.5763, Avg Loss: 3.3125
Epoch [2/2], Batch [40/100] (40.0%), Loss: 3.5513, Avg Loss: 3.3120
Epoch [2/2], Batch [45/100] (45.0%), Loss: 3.3792, Avg Loss: 3.3244
Epoch [2/2], Batch [50/100] (50.0%), Loss: 3.1900, Avg Loss: 3.3072
Epoch [2/2], Batch [55/100] (55.0%), Loss: 3.3707, Avg Loss: 3.3378
Epoch [2/2], Batch [60/100] (60.0%), Loss: 3.3086, Avg Loss: 3.3445
Epoch [2/2], Batch [65/100] (65.0%), Loss: 3.4028, Avg Loss: 3.3401
Epoch [2/2], Batch [70/100] (70.0%), Loss: 2.7725, Avg Loss: 3.3218
Epoch [2/2], Batch [75/100] (75.0%), Loss: 3.1985, Avg Loss: 3.3095
Epoch [2/2], Batch [80/100] (80.0%), Loss: 3.4619, Avg Loss: 3.3047
Epoch [2/2], Batch [85/100] (85.0%), Loss: 3.3617, Avg Loss: 3.2993
Epoch [2/2], Batch [90/100] (90.0%), Loss: 3.2596, Avg Loss: 3.2965
Epoch [2/2], Batch [95/100] (95.0%), Loss: 3.3602, Avg Loss: 3.2927
Epoch [2/2], Batch [100/100] (100.0%), Loss: 2.6229, Avg Loss: 3.2792

==================================================
EPOCH 2/2 COMPLETED
Average Training Loss: 3.2792
Processed Batches: 100/100
Success Rate: 100.0%
==================================================
Debug: Max score: 0.2693, Valid predictions: 100
Validation - Loss: 3.2621, mAP: 0.0000
Stage 2 Summary:
  Train Loss: 3.2792
  Val Loss: 3.2621
  Val mAP: 0.0000
  Best mAP: 0.0000
  Learning Rate: 0.000100
  Elapsed Time: 0.02h

==================================================
Two-stage training completed in 0.02 hours
Final validation mAP: 0.0000
Stage 1 (heads): 1 epochs
Stage 2 (all): 1 epochs
==================================================
